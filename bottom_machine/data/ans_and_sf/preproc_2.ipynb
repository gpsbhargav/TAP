{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import pdb\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import (BasicTokenizer,\n",
    "                                                  BertTokenizer,\n",
    "                                                  whitespace_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickler(path,pkl_name,obj):\n",
    "    with open(os.path.join(path, pkl_name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def unpickler(path,pkl_name):\n",
    "    with open(os.path.join(path, pkl_name) ,'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = True\n",
    "\n",
    "out_pkl_path = \"./\"\n",
    "in_pkl_path = \"./\"\n",
    "\n",
    "if(TRAINING):\n",
    "    in_pkl_name = \"preproc_train_1.pkl\"\n",
    "    out_pkl_name = \"preprocessed_train.pkl\"\n",
    "    small_out_pkl_name = \"preprocessed_train_small.pkl\"\n",
    "    small_dataset_size = 5000\n",
    "else:\n",
    "    in_pkl_name = \"preproc_dev_1.pkl\"\n",
    "    out_pkl_name = \"preprocessed_dev.pkl\"\n",
    "    small_out_pkl_name = \"preprocessed_dev_small.pkl\"\n",
    "    small_dataset_size = 500\n",
    "\n",
    "max_seq_len = 512\n",
    "max_question_len = 35\n",
    "max_context_chunk_length = max_seq_len - max_question_len - 2\n",
    "max_num_chunks = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] : 101\n",
      "[SEP] : 102\n",
      "[PAD] : 0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "cls_id = tokenizer.convert_tokens_to_ids([\"[CLS]\"])[0]\n",
    "sep_id = tokenizer.convert_tokens_to_ids([\"[SEP]\"])[0]\n",
    "pad_id = tokenizer.convert_tokens_to_ids([\"[PAD]\"])[0]\n",
    "                                         \n",
    "\n",
    "print(\"[CLS] : {}\".format(cls_id))\n",
    "print(\"[SEP] : {}\".format(sep_id))\n",
    "print(\"[PAD] : {}\".format(pad_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = unpickler(in_pkl_path, in_pkl_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question_ids', 'questions', 'paragraphs', 'paragraph_names', 'answers', 'question_indices', 'yes_no_span', 'supporting_facts', 'ids_to_word_mappings', 'answers_string', 'supporting_facts_raw'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_paragraph_length(paragraph):\n",
    "    length = 0\n",
    "    for sent in paragraph:\n",
    "        length += len(sent)\n",
    "    return length\n",
    "\n",
    "def make_chunks(paragraphs, max_num_chunks, max_chuck_length):\n",
    "    chunks = [[] for i in range(max_num_chunks)]\n",
    "    paragraph_lengths = [compute_paragraph_length(p) for p in paragraphs]\n",
    "    paragraph_indices_arg_sorted = np.argsort(paragraph_lengths)\n",
    "    num_sentences_in_paragraphs = [len(p) for p in paragraphs]\n",
    "    paragraph_chunk_indices = [[] for i in range(max_num_chunks)]\n",
    "    chunk_len_so_far = 0\n",
    "    current_chunk=0\n",
    "    for i in paragraph_indices_arg_sorted:\n",
    "        if(chunk_len_so_far + min(max_chuck_length, paragraph_lengths[i]) > max_chuck_length):\n",
    "            if(current_chunk < max_num_chunks-1):\n",
    "                current_chunk += 1\n",
    "                chunk_len_so_far = 0\n",
    "        paragraph_chunk_indices[current_chunk].append(i)\n",
    "        chunks[current_chunk] += paragraphs[i]\n",
    "        chunk_len_so_far += paragraph_lengths[i]\n",
    "        \n",
    "    return chunks, paragraph_chunk_indices, num_sentences_in_paragraphs\n",
    "    \n",
    "def reorganize_supporting_fact_labels(paragraph_chunk_indices, num_sentences_in_paragraphs, supporting_facts):\n",
    "    def find_new_index(paragraph_chunk_indices, sf):\n",
    "        for i,chunk in enumerate(paragraph_chunk_indices):\n",
    "            if sf[0] in chunk:\n",
    "                sentence_index = sum([num_sentences_in_paragraphs[j] for j in chunk[:chunk.index(sf[0])] ]) + sf[1]\n",
    "                return [i, sentence_index]\n",
    "    \n",
    "    sf_out = []\n",
    "    \n",
    "    for sf in supporting_facts:\n",
    "        chunk_index, sentence_index = find_new_index(paragraph_chunk_indices, sf)\n",
    "        sf_out.append([chunk_index, sentence_index])\n",
    "    \n",
    "    return sf_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trim(sequences, max_len, pad_symbol=0):\n",
    "    sequences_out = []\n",
    "    for sequence in sequences:\n",
    "        seq = sequence[:max_len]\n",
    "        seq += [pad_symbol] * (max_len - len(seq))\n",
    "        sequences_out.append(seq)\n",
    "    return sequences_out\n",
    "\n",
    "def trim_paragraph(paragraph, max_seq_len):\n",
    "    assert(max_seq_len >= 0)\n",
    "    sent_lengths = [len(s) for s in paragraph]\n",
    "    out_paragraph = []\n",
    "    length_so_far = 0\n",
    "    for sent in paragraph:\n",
    "        if(len(sent) == 0):\n",
    "            continue\n",
    "        if(length_so_far + len(sent) <= max_seq_len):\n",
    "            out_paragraph.append(sent)\n",
    "            if(length_so_far + len(sent) == max_seq_len):\n",
    "                break\n",
    "            length_so_far += len(sent)      \n",
    "        else:\n",
    "            sent = sent[:max_seq_len-length_so_far]\n",
    "            out_paragraph.append(sent)\n",
    "            break\n",
    "    return out_paragraph\n",
    "    \n",
    "def pad_paragraph(paragraph, max_sequence_len, pad_index):\n",
    "    assert(max_sequence_len >= 0)\n",
    "    sent_lengths = [len(s) for s in paragraph]\n",
    "    assert(sum(sent_lengths) <= max_sequence_len)\n",
    "    paragraph.append([pad_index] * (max_sequence_len - sum(sent_lengths)))\n",
    "    return paragraph\n",
    "\n",
    "def merge_trim_pad_paragraphs(paragraph, paragraph_index, supporting_facts_in, max_seq_len,\n",
    "                              max_sentences, pad_index=0):\n",
    "    sentence_start_indices = []\n",
    "    sentence_end_indices = []\n",
    "    \n",
    "    paragraph = paragraph[:max_sentences-1]\n",
    "    \n",
    "    total_para_len_words = sum([len(s) for s in paragraph])\n",
    "    \n",
    "    available_length_for_paragraph = max_seq_len\n",
    "    \n",
    "    if(total_para_len_words >= available_length_for_paragraph):\n",
    "        paragraph = trim_paragraph(paragraph, available_length_for_paragraph-1) #-1 to make room for the next empty sentence\n",
    "        paragraph.append([pad_index])\n",
    "    elif(total_para_len_words < available_length_for_paragraph):\n",
    "        paragraph = pad_paragraph(paragraph, available_length_for_paragraph, pad_index)\n",
    "        \n",
    "        \n",
    "    #concatenate sentences, note starting and ending indices of sentences\n",
    "    sentence_start_indices = []\n",
    "    sentence_end_indices = []\n",
    "    out_sequence = []\n",
    "    for sent in paragraph:\n",
    "        sentence_start_indices.append(len(out_sequence))\n",
    "        out_sequence += sent\n",
    "        sentence_end_indices.append(len(out_sequence)-1)\n",
    "    \n",
    "    assert(len(sentence_start_indices) == len(sentence_end_indices))\n",
    "            \n",
    "    #create supporting_facts vector\n",
    "    supporting_facts = [0] * max_sentences\n",
    "    for s_f in supporting_facts_in:\n",
    "        if(s_f[0] == paragraph_index and s_f[1]<max_sentences):\n",
    "            supporting_facts[s_f[1]] = 1\n",
    "            \n",
    "            \n",
    "    # sanity check\n",
    "    assert(len(out_sequence) == available_length_for_paragraph)\n",
    "    assert(len(supporting_facts) == max_sentences)\n",
    "    \n",
    "    return {'sequence': out_sequence,\n",
    "            'sentence_start_index': sentence_start_indices, 'sentence_end_index': sentence_end_indices,\n",
    "            'supporting_fact': supporting_facts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_in_sequence(sequence, key):\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    for i in range(len(sequence)):\n",
    "        if(sequence[i:i+len(key)] == key):\n",
    "            start_indices.append(i)\n",
    "            end_indices.append(i+len(key)-1)\n",
    "    assert(len(start_indices) == len(end_indices))\n",
    "    return start_indices,end_indices\n",
    "\n",
    "def find_answer_locations(passages, answers, yes_no_span):\n",
    "    '''\n",
    "    Input: [Passage_chunk_0 , ... , Passage_chunk_j ] .  \n",
    "    Finds the indices of true answer in the chunks. The indices are as if the passage chunks are concatenated \n",
    "    '''\n",
    "    assert(len(passages) == len(answers))\n",
    "    answer_start_indices = []\n",
    "    answer_end_indices = []\n",
    "    for i in range(len(passages)):\n",
    "        if(yes_no_span[i] != 2):\n",
    "            answer_start_indices.append([0])\n",
    "            answer_end_indices.append([0])\n",
    "        else:\n",
    "            mega_sequence = []\n",
    "            for j in range(len(passages[i])):\n",
    "                mega_sequence += passages[i][j]\n",
    "            a_s, a_e = find_all_in_sequence(mega_sequence, answers[i])\n",
    "            assert(len(a_s) == len(a_e))\n",
    "            answer_start_indices.append(a_s)\n",
    "            answer_end_indices.append(a_e)\n",
    "    return answer_start_indices, answer_end_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks, paragraph_chunk_indices, num_sentences_in_paragraphs = make_chunks(paragraphs=data_in['paragraphs'][0], \n",
    "                                                                           max_num_chunks=max_num_chunks, \n",
    "                                                                           max_chuck_length=max_context_chunk_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 4, 9, 4, 1, 3, 5, 4, 4, 5]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sentences_in_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 7, 5, 8, 9, 6], [3, 1, 0], [2], []]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_chunk_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(chunks[0]))\n",
    "# print(len(chunks[1]))\n",
    "# print(len(chunks[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 2], [0, 3], [0, 7], [2, 1]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reorganize_supporting_fact_labels(paragraph_chunk_indices=[[0,2],[1],[3]], num_sentences_in_paragraphs=[5,2,3,2], \n",
    "                                  supporting_facts = [[0,2],[0,3],[2,2],[3,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90447/90447 [00:18<00:00, 4765.69it/s]\n"
     ]
    }
   ],
   "source": [
    "paragraphs_chunked = []\n",
    "paragraph_chunk_indices = []\n",
    "num_sentences_in_paragraphs = []\n",
    "\n",
    "for i in trange(len(data_in['paragraphs'])):\n",
    "    chunk, p_indices, num_sentences = make_chunks(paragraphs=data_in['paragraphs'][i], \n",
    "                                                  max_num_chunks=max_num_chunks, \n",
    "                                                  max_chuck_length=max_context_chunk_length)\n",
    "    paragraphs_chunked.append(chunk)\n",
    "    paragraph_chunk_indices.append(p_indices)\n",
    "    num_sentences_in_paragraphs.append(num_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90447/90447 [00:00<00:00, 136028.33it/s]\n"
     ]
    }
   ],
   "source": [
    "supporting_facts_in_paragraph_chunks = []\n",
    "\n",
    "for i in trange(len(data_in['paragraphs'])):\n",
    "    sf = reorganize_supporting_fact_labels(paragraph_chunk_indices= paragraph_chunk_indices[i], \n",
    "                                           num_sentences_in_paragraphs= num_sentences_in_paragraphs[i], \n",
    "                                           supporting_facts = data_in['supporting_facts'][i])\n",
    "    supporting_facts_in_paragraph_chunks.append(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0], [0, 1], [0, 11]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supporting_facts_in_paragraph_chunks[4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs_chunked[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg number of sentences per chunk :10.236226740522074\n",
      "min number of sentences per chunk :0\n",
      "max number of sentences per chunk :103\n"
     ]
    }
   ],
   "source": [
    "num_sentences_per_chunk = []\n",
    "\n",
    "for p_chunks in paragraphs_chunked:\n",
    "    for chunk in p_chunks:\n",
    "        num_sentences_per_chunk.append(len(chunk))\n",
    "        \n",
    "num_sentences_per_chunk = np.array(num_sentences_per_chunk)\n",
    "\n",
    "print(\"Avg number of sentences per chunk :{}\".format(num_sentences_per_chunk.mean()))\n",
    "print(\"min number of sentences per chunk :{}\".format(num_sentences_per_chunk.min()))\n",
    "print(\"max number of sentences per chunk :{}\".format(num_sentences_per_chunk.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04600760666467655"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_num_sentences_per_chunk = 18\n",
    "np.sum(np.greater(num_sentences_per_chunk,max_num_sentences_per_chunk))/num_sentences_per_chunk.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 0], [9, 0]]\n",
      "[[1, 4], [1, 7]]\n",
      "[[0, 2, 6, 1, 7, 4, 3], [8, 5, 9], [], []]\n"
     ]
    }
   ],
   "source": [
    "i=990\n",
    "print(data_in['supporting_facts'][i])\n",
    "print(supporting_facts_in_paragraph_chunks[i])\n",
    "print(paragraph_chunk_indices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_fixed_length = pad_trim(sequences = data_in['questions'], max_len=max_question_len, pad_symbol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_in['questions'])):\n",
    "    assert(len(questions_fixed_length[i]) == max_question_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90447/90447 [00:14<00:00, 6419.27it/s]\n"
     ]
    }
   ],
   "source": [
    "paragraph_chunks_fixed_length = []\n",
    "sentence_start_indices = []\n",
    "sentence_end_indices = []\n",
    "supporting_facts_expanded = []\n",
    "\n",
    "\n",
    "for i,q in enumerate(tqdm(questions_fixed_length)):\n",
    "    p_chunks = []\n",
    "    sent_start = []\n",
    "    sent_end = []\n",
    "    sf_expanded = []\n",
    "    for j,para in enumerate(paragraphs_chunked[i]):\n",
    "        info = merge_trim_pad_paragraphs(paragraph=para, paragraph_index=j, \n",
    "                                          supporting_facts_in=supporting_facts_in_paragraph_chunks[i], \n",
    "                                          max_seq_len=max_seq_len-max_question_len-2,\n",
    "                                          max_sentences=max_num_sentences_per_chunk, pad_index=pad_id)\n",
    "        p_chunks.append(info['sequence'])\n",
    "        sent_start.append(info['sentence_start_index'])\n",
    "        sent_end.append(info['sentence_end_index'])\n",
    "        sf_expanded += info['supporting_fact']\n",
    "    \n",
    "    paragraph_chunks_fixed_length.append(p_chunks)\n",
    "    sentence_start_indices.append(sent_start)\n",
    "    sentence_end_indices.append(sent_end)\n",
    "    supporting_facts_expanded.append(sf_expanded)\n",
    "    \n",
    "\n",
    "assert(len(paragraph_chunks_fixed_length) == len(sentence_start_indices))\n",
    "assert(len(sentence_end_indices) == len(sentence_start_indices))\n",
    "assert(len(supporting_facts_expanded) == len(sentence_start_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(supporting_facts_expanded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 12], [0, 0]]\n",
      "[ 0 12]\n"
     ]
    }
   ],
   "source": [
    "idx = 3601\n",
    "print(supporting_facts_in_paragraph_chunks[idx])\n",
    "print(np.where(np.array(supporting_facts_expanded[idx]) == 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(supporting_facts_expanded)):\n",
    "    assert(len(supporting_facts_expanded[i]) == max_num_sentences_per_chunk * max_num_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90447\n"
     ]
    }
   ],
   "source": [
    "print(len(paragraph_chunks_fixed_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475\n",
      "475\n"
     ]
    }
   ],
   "source": [
    "print(max_seq_len-max_question_len-2)\n",
    "print(len(paragraph_chunks_fixed_length[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_start_indices, answer_end_indices = find_answer_locations(passages = paragraph_chunks_fixed_length, \n",
    "                                                                 answers = data_in['answers'], \n",
    "                                                                 yes_no_span = data_in['yes_no_span'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(supporting_facts_expanded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(answer_start_indices) == len(answer_end_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(answer_start_indices)):\n",
    "    assert(len(answer_start_indices[i]) == len(answer_end_indices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63, 369, 1158]\n",
      "[65, 371, 1160]\n"
     ]
    }
   ],
   "source": [
    "i = 344\n",
    "print(answer_start_indices[i])\n",
    "print(answer_end_indices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 31, 65, 104, 155, 178, 214, 239, 264, 279, 351, 401, 423, 449]\n",
      "[30, 64, 103, 154, 177, 213, 238, 263, 278, 350, 400, 422, 448, 474]\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "print(sentence_start_indices[i][0])\n",
    "print(sentence_end_indices[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90447/90447 [00:04<00:00, 19813.86it/s]\n"
     ]
    }
   ],
   "source": [
    "assert(len(paragraph_chunks_fixed_length) == len(questions_fixed_length))\n",
    "\n",
    "question_context_sequences = []\n",
    "for i in trange(len(paragraph_chunks_fixed_length)):\n",
    "    sequences = []\n",
    "    for j in range(len(paragraph_chunks_fixed_length[i])):\n",
    "        seq = [cls_id] + questions_fixed_length[i] + [sep_id] + paragraph_chunks_fixed_length[i][j]\n",
    "        sequences.append(seq)\n",
    "    question_context_sequences.append(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(question_context_sequences)):\n",
    "    for j in range(len(question_context_sequences[i])):\n",
    "        assert(len(question_context_sequences[i][j]) == max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_id = [0] + [0]*max_question_len + [1] + [1]* (max_seq_len - max_question_len - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(segment_id) == max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to pkl:\n",
    "- question_context_sequences\n",
    "- segment_id\n",
    "- sentence_start_indices\n",
    "- sentence_end_indices\n",
    "- answer_start_indices\n",
    "- answer_end_indices\n",
    "- supporting_facts_expanded\n",
    "- question_ids\n",
    "- question_indices\n",
    "- yes_no_span\n",
    "- ids_to_word_mappings\n",
    "- max_seq_len\n",
    "- max_question_len\n",
    "- paragraph_chunk_indices\n",
    "- num_sentences_in_paragraphs\n",
    "- paragraph_names\n",
    "- answers_string\n",
    "- supporting_facts_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(\n",
    "    len(question_context_sequences) == \n",
    "    len(sentence_start_indices) == \n",
    "    len(sentence_end_indices) == \n",
    "    len(answer_start_indices) == \n",
    "    len(answer_end_indices) == \n",
    "    len(data_in['question_ids']) == \n",
    "    len(data_in['question_indices']) == \n",
    "    len(data_in['yes_no_span']) == \n",
    "    len(data_in['ids_to_word_mappings']) == \n",
    "    len(paragraph_chunk_indices) == \n",
    "    len(data_in['paragraph_names']) == \n",
    "    len(data_in['answers_string']) == \n",
    "    len(data_in['supporting_facts_raw']) == \n",
    "    len(supporting_facts_expanded) == \n",
    "    len(num_sentences_in_paragraphs) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {\n",
    "    'question_context_sequences': question_context_sequences,\n",
    "    'segment_id' : segment_id,\n",
    "    'sentence_start_indices': sentence_start_indices,\n",
    "    'sentence_end_indices': sentence_end_indices,\n",
    "    'answer_start_indices': answer_start_indices,\n",
    "    'answer_end_indices':answer_end_indices,\n",
    "    'supporting_facts_expanded': supporting_facts_expanded,\n",
    "    'question_ids': data_in['question_ids'],\n",
    "    'question_indices': data_in['question_indices'],\n",
    "    'yes_no_span': data_in['yes_no_span'],\n",
    "    'ids_to_word_mappings': data_in['ids_to_word_mappings'],\n",
    "    'max_seq_len': max_seq_len,\n",
    "    'max_question_len': max_question_len,\n",
    "    'max_num_sentences_per_chunk': max_num_sentences_per_chunk,\n",
    "    'num_chunks': len(question_context_sequences[0]),\n",
    "    'paragraph_chunk_indices': paragraph_chunk_indices,\n",
    "    'num_sentences_in_paragraphs': num_sentences_in_paragraphs,\n",
    "    'paragraph_names': data_in['paragraph_names'],\n",
    "    'answers_string': data_in['answers_string'],\n",
    "    'supporting_facts_raw': data_in['supporting_facts_raw']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_out_dict = {\n",
    "    'question_context_sequences': question_context_sequences[:small_dataset_size],\n",
    "    'segment_id' : segment_id,\n",
    "    'sentence_start_indices': sentence_start_indices[:small_dataset_size],\n",
    "    'sentence_end_indices': sentence_end_indices[:small_dataset_size],\n",
    "    'answer_start_indices': answer_start_indices[:small_dataset_size],\n",
    "    'answer_end_indices':answer_end_indices[:small_dataset_size],\n",
    "    'supporting_facts_expanded': supporting_facts_expanded[:small_dataset_size],\n",
    "    'question_ids': data_in['question_ids'][:small_dataset_size],\n",
    "    'question_indices': data_in['question_indices'][:small_dataset_size],\n",
    "    'yes_no_span': data_in['yes_no_span'][:small_dataset_size],\n",
    "    'ids_to_word_mappings': data_in['ids_to_word_mappings'][:small_dataset_size],\n",
    "    'max_seq_len': max_seq_len,\n",
    "    'max_question_len': max_question_len,\n",
    "    'max_num_sentences_per_chunk': max_num_sentences_per_chunk,\n",
    "    'num_chunks': len(question_context_sequences[0]),\n",
    "    'paragraph_chunk_indices': paragraph_chunk_indices[:small_dataset_size],\n",
    "    'num_sentences_in_paragraphs': num_sentences_in_paragraphs[:small_dataset_size],\n",
    "    'paragraph_names': data_in['paragraph_names'][:small_dataset_size],\n",
    "    'answers_string': data_in['answers_string'][:small_dataset_size],\n",
    "    'supporting_facts_raw': data_in['supporting_facts_raw'][:small_dataset_size]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "pickler(out_pkl_path, out_pkl_name, out_dict)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "pickler(out_pkl_path, small_out_pkl_name, small_out_dict)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
