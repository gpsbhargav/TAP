{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing HotpotQA\n",
    "- Form paragraphs only out of the supporting facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import pdb\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import (BasicTokenizer,\n",
    "                                                  BertTokenizer,\n",
    "                                                  whitespace_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickler(path,pkl_name,obj):\n",
    "    with open(os.path.join(path, pkl_name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def unpickler(path,pkl_name):\n",
    "    with open(os.path.join(path, pkl_name) ,'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] : 101\n",
      "[SEP] : 102\n",
      "[PAD] : 0\n"
     ]
    }
   ],
   "source": [
    "cls_id = tokenizer.convert_tokens_to_ids([\"[CLS]\"])[0]\n",
    "sep_id = tokenizer.convert_tokens_to_ids([\"[SEP]\"])[0]\n",
    "pad_id = tokenizer.convert_tokens_to_ids([\"[PAD]\"])[0]\n",
    "                                         \n",
    "\n",
    "print(\"[CLS] : {}\".format(cls_id))\n",
    "print(\"[SEP] : {}\".format(sep_id))\n",
    "print(\"[PAD] : {}\".format(pad_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = True\n",
    "\n",
    "out_pkl_path = \"./\"\n",
    "\n",
    "if(TRAINING):\n",
    "    file_path = \"../../../hotpotqa/hotpot_train_v1.1.json\"\n",
    "    out_pkl_name = \"preprocessed_train.pkl\"\n",
    "    small_out_pkl_name = \"preprocessed_train_small.pkl\"\n",
    "    small_dataset_size = 5000\n",
    "else:\n",
    "    file_path = \"../../../hotpotqa/hotpot_dev_distractor_v1.json\"\n",
    "    out_pkl_name = \"preprocessed_dev_0.4.pkl\"\n",
    "    small_out_pkl_name = \"preprocessed_dev_small.pkl\"\n",
    "    small_dataset_size = 500\n",
    "\n",
    "pred_pkl_path = '../pred_for_threshold/'\n",
    "pred_pkl_name = 't_0.4'\n",
    "\n",
    "max_seq_len = 510\n",
    "max_num_paragraphs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not TRAINING):\n",
    "    predictions = unpickler(pred_pkl_path, pred_pkl_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not TRAINING):\n",
    "    predictions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not TRAINING):\n",
    "    list(predictions['sp'].keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not TRAINING):\n",
    "    type(predictions['sp']['5a8b57f25542995d1e6f1371'][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, encoding='utf8') as file:\n",
    "    dataset = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, tokens_to_text_mapping, bert_tokenizer):\n",
    "    out_list = []\n",
    "    tokens = whitespace_tokenize(text)\n",
    "    for tok in tokens:\n",
    "        ids = bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize(tok))\n",
    "        tokens_to_text_mapping[tuple(ids)] = tok\n",
    "        out_list += ids\n",
    "    return out_list\n",
    "\n",
    "def un_tokenize(ids, tokens_to_text_mapping, bert_tokenizer):\n",
    "    out_list = []\n",
    "    start = 0\n",
    "    end = start\n",
    "    # remove \" from the list because it causes confusion between Book \"shelf ... \n",
    "    # and Book\" shelf ... while un-tokenizing\n",
    "    # ids = list(filter(lambda x: x != 1000, ids)) \n",
    "    while (start < len(ids)) and (end < len(ids)):\n",
    "        i = len(ids)\n",
    "        decoded_anything = False\n",
    "        while (decoded_anything == False) and (i > start):\n",
    "            if(tuple(ids[start:i]) in tokens_to_text_mapping.keys()):\n",
    "                out_list.append(tokens_to_text_mapping[tuple(ids[start:i])])\n",
    "                decoded_anything = True\n",
    "            else:\n",
    "                i -= 1\n",
    "        if(decoded_anything == False):\n",
    "            start += 1\n",
    "            end = start\n",
    "        else:\n",
    "            start = i\n",
    "            end = i\n",
    "    return \" \".join(out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90447/90447 [05:26<00:00, 277.26it/s]\n"
     ]
    }
   ],
   "source": [
    "question_ids = []\n",
    "questions = []\n",
    "paragraphs = [] \n",
    "answers = []\n",
    "answers_string = []\n",
    "question_indices = []\n",
    "yes_no_span = []\n",
    "supporting_facts = []\n",
    "ids_to_word_mappings = []\n",
    "\n",
    "skipped = []\n",
    "\n",
    "for item_index, item in enumerate(tqdm(dataset)):\n",
    "    answers_string.append(item[\"answer\"])\n",
    "    id_to_word = {}\n",
    "    paragraph_names = []\n",
    "    for i,para in enumerate(item[\"context\"]):\n",
    "        para_name = para[0]\n",
    "        paragraph_names.append(para_name)\n",
    "    supp_fact_list = []\n",
    "    \n",
    "    if(TRAINING):\n",
    "        supporting_facts_for_this_question = item[\"supporting_facts\"]\n",
    "    else:\n",
    "        supporting_facts_for_this_question = predictions[item[\"_id\"]]\n",
    "        \n",
    "    for sup_fact in supporting_facts_for_this_question:\n",
    "        para_name = sup_fact[0]\n",
    "        supporting_fact_index = sup_fact[1] \n",
    "        para_index = paragraph_names.index(para_name)\n",
    "        supp_fact_list.append([para_index, supporting_fact_index])\n",
    "\n",
    "    gold_paragraphs = []\n",
    "    reorganized_sf_list = []\n",
    "    for p_index, supporting_para in enumerate(set([para_index for para_index, supporting_fact_index in supp_fact_list])):\n",
    "        para = item[\"context\"][supporting_para]\n",
    "        para_name = para[0]\n",
    "        para_sents = para[1]\n",
    "        para_sents[0] = para_name + \". \" +para_sents[0]\n",
    "        gold_paragraphs.append([tokenize(s, id_to_word, tokenizer) for s in para_sents])\n",
    "        supporting_facts_in_this_para = []\n",
    "        for para_index, supporting_fact_index in supp_fact_list:\n",
    "            if(para_index == supporting_para):\n",
    "                supporting_facts_in_this_para.append(supporting_fact_index)\n",
    "        reorganized_sf_list.append(supporting_facts_in_this_para)\n",
    "    \n",
    "    supporting_facts.append(reorganized_sf_list)\n",
    "    paragraphs.append(gold_paragraphs)\n",
    "    question_indices.append(item_index)\n",
    "    question_ids.append(item[\"_id\"])\n",
    "    question = tokenize(item[\"question\"], id_to_word, tokenizer)\n",
    "    questions.append(question)\n",
    "    answer_str = item[\"answer\"]\n",
    "    if(answer_str == \"yes\"):\n",
    "        yes_no_span.append(0)\n",
    "    elif(answer_str == \"no\"):\n",
    "        yes_no_span.append(1)\n",
    "    else:\n",
    "        yes_no_span.append(2)\n",
    "    answer_tokenized = tokenize(answer_str, {}, tokenizer)\n",
    "    answers.append(answer_tokenized)\n",
    "    ids_to_word_mappings.append(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [0]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supporting_facts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90447\n",
      "90447\n",
      "90447\n"
     ]
    }
   ],
   "source": [
    "print(len(paragraphs))\n",
    "print(len(answers))\n",
    "print(len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90447\n"
     ]
    }
   ],
   "source": [
    "print(len(answers_string)) #unfiltered list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 examples\n"
     ]
    }
   ],
   "source": [
    "print(\"Skipped {} examples\".format(len(skipped)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([len(p) for p in paragraphs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [0]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supporting_facts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min question length: 4\n",
      "Avg question length: 22.443585746348692\n",
      "Max question length: 141\n"
     ]
    }
   ],
   "source": [
    "question_lengths = np.array([len(q) for q in questions])\n",
    "print(\"Min question length: {}\".format(question_lengths.min()))\n",
    "print(\"Avg question length: {}\".format(question_lengths.mean()))\n",
    "print(\"Max question length: {}\".format(question_lengths.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07017369288091369"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_question_len = 40\n",
    "np.sum(np.greater(question_lengths,max_question_len))/question_lengths.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min combined_gold_para_lengths: 38\n",
      "Avg combined_gold_para_lengths: 197.25920152133293\n",
      "Max combined_gold_para_lengths: 806\n"
     ]
    }
   ],
   "source": [
    "combined_gold_para_lengths = []\n",
    "for para_list in paragraphs:\n",
    "    length = 0\n",
    "    for para in para_list:\n",
    "        for sentence in para:\n",
    "            length += len(sentence)\n",
    "    combined_gold_para_lengths.append(length)\n",
    "\n",
    "combined_gold_para_lengths = np.array(combined_gold_para_lengths)\n",
    "\n",
    "print(\"Min combined_gold_para_lengths: {}\".format(combined_gold_para_lengths.min()))\n",
    "print(\"Avg combined_gold_para_lengths: {}\".format(combined_gold_para_lengths.mean()))\n",
    "print(\"Max combined_gold_para_lengths: {}\".format(combined_gold_para_lengths.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max passage length:  468\n",
      "0.0030957356241776953\n"
     ]
    }
   ],
   "source": [
    "max_passage_length = max_seq_len - max_question_len - 2\n",
    "print(\"max passage length: \",max_passage_length)\n",
    "print(np.sum(np.greater(combined_gold_para_lengths,max_passage_length))/combined_gold_para_lengths.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trim(sequences, max_len, pad_symbol=0):\n",
    "    sequences_out = []\n",
    "    for sequence in sequences:\n",
    "        seq = sequence[:max_len]\n",
    "        seq += [pad_symbol] * (max_len - len(seq))\n",
    "        sequences_out.append(seq)\n",
    "    return sequences_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_fixed_len = pad_trim(questions, max_question_len, pad_symbol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{40}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([len(q) for q in questions_fixed_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_in_sentence(sequence, key):\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    for i in range(len(sequence)):\n",
    "        if(sequence[i:i+len(key)] == key):\n",
    "            start_indices.append(i)\n",
    "            end_indices.append(i+len(key)-1)\n",
    "    assert(len(start_indices) == len(end_indices))\n",
    "    return start_indices,end_indices\n",
    "\n",
    "def find_answer_locations(passages, answers, yes_no_span):\n",
    "    assert(len(passages) == len(answers))\n",
    "    answer_start_indices = []\n",
    "    answer_end_indices = []\n",
    "    for i in range(len(passages)):\n",
    "        if(yes_no_span[i] != 2):\n",
    "            answer_start_indices.append([0])\n",
    "            answer_end_indices.append([0])\n",
    "        else:    \n",
    "            s, e = find_all_in_sentence(passages[i], answers[i])\n",
    "            assert(len(s) == len(e))\n",
    "            answer_start_indices.append(s)\n",
    "            answer_end_indices.append(e)\n",
    "    return answer_start_indices, answer_end_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_para_out_of_supporting_facts(passages, supporting_facts):\n",
    "    skipped = []\n",
    "    out_passages = []\n",
    "    assert(len(passages) == len(supporting_facts))\n",
    "    for i in range(len(passages)):\n",
    "        new_passage = []\n",
    "        for j,passage in enumerate(passages[i]):\n",
    "            for s_f in supporting_facts[i][j]:\n",
    "                try:\n",
    "                    new_passage += passage[s_f]\n",
    "                except:\n",
    "                    skipped.append(i)\n",
    "        out_passages.append(new_passage)\n",
    "    return out_passages, skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['Para1', 'Sentence1', 'Para2', 'Sentence2']], [])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_para_out_of_supporting_facts(passages= [[[[\"Para1\", \"Sentence1\"],[\"Para1\", \"Sentence2\"]],\n",
    "                                  [[\"Para2\", \"Sentence1\"],[\"Para2\", \"Sentence2\"]]]], supporting_facts=[[[0],[1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_sf_only, skipped_sf_index_out_of_range = make_para_out_of_supporting_facts(passages=paragraphs, supporting_facts=supporting_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(skipped_sf_index_out_of_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90447"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs_sf_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs_sf_only[2050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_length_context = pad_trim(paragraphs_sf_only, max_passage_length, pad_symbol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_start_indices, answer_end_indices = find_answer_locations(fixed_length_context, answers=answers, yes_no_span=yes_no_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(answer_start_indices) == len(answer_end_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90447"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90447"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90447"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer_start_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of span types where answer string is not found in context: 98\n",
      "Min occurrences of answer in gold context: 1\n",
      "Avg occurrences of answer in gold context: 1.565525286327002\n",
      "Max occurrences of answer in gold context: 16\n"
     ]
    }
   ],
   "source": [
    "num_occurrences_of_answer = []\n",
    "question_indices_without_answer = []\n",
    "for i in range(len(answer_start_indices)):\n",
    "    if(yes_no_span[i] == 2):\n",
    "        if(len(answer_start_indices[i]) == 0):\n",
    "            question_indices_without_answer.append(i)\n",
    "        else:\n",
    "            num_occurrences_of_answer.append(len(answer_start_indices[i]))\n",
    "\n",
    "num_occurrences_of_answer = np.array(num_occurrences_of_answer)\n",
    "\n",
    "print(\"Number of span types where answer string is not found in context: {}\".format(len(question_indices_without_answer)))\n",
    "\n",
    "print(\"Min occurrences of answer in gold context: {}\".format(num_occurrences_of_answer.min()))\n",
    "print(\"Avg occurrences of answer in gold context: {}\".format(num_occurrences_of_answer.mean()))\n",
    "print(\"Max occurrences of answer in gold context: {}\".format(num_occurrences_of_answer.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104]\n",
      "[106]\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "print(answer_start_indices[i])\n",
    "print(answer_end_indices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING:\n",
    "    for i in range(len(questions)-1, -1, -1):\n",
    "        if(i in question_indices_without_answer or i in skipped_sf_index_out_of_range):\n",
    "            del(question_ids[i])\n",
    "            del(questions_fixed_len[i])\n",
    "            del(fixed_length_context[i])\n",
    "            del(question_indices[i])\n",
    "            del(yes_no_span[i])\n",
    "            del(answer_start_indices[i])\n",
    "            del(answer_end_indices[i])\n",
    "            del(ids_to_word_mappings[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_id = [0] + [0]*max_question_len + [1] + [1]*max_passage_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(segment_id) == max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_context_sequences = []\n",
    "for i in range(len(questions_fixed_len)):\n",
    "    seq = [cls_id] + questions_fixed_len[i] + [sep_id] + fixed_length_context[i]\n",
    "    question_context_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq in question_context_sequences:\n",
    "    assert(len(seq) == max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offset all answer pointers by max_question_len + 2\n",
    "\n",
    "answer_start_indices_offset = []\n",
    "answer_end_indices_offset = []\n",
    "\n",
    "for i in range(len(answer_end_indices)):\n",
    "    start = []\n",
    "    end = []\n",
    "    assert(len(answer_end_indices[i]) == len(answer_start_indices[i]))\n",
    "    for j in range(len(answer_end_indices[i])):\n",
    "        start.append(answer_start_indices[i][j] + max_question_len + 2)\n",
    "        end.append(answer_end_indices[i][j] + max_question_len + 2)\n",
    "    answer_start_indices_offset.append(start)\n",
    "    answer_end_indices_offset.append(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[146]\n",
      "[148]\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "print(answer_start_indices_offset[i])\n",
    "print(answer_end_indices_offset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to pkl:\n",
    "- question_context_sequences\n",
    "- question_ids\n",
    "- question_indices\n",
    "- yes_no_span\n",
    "- answer_start_indices_offset\n",
    "- answer_end_indices_offset\n",
    "- max_question_len\n",
    "- max_seq_len\n",
    "- segment_id\n",
    "- ids_to_word_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_context_sequences: 90327\n",
      "question_ids: 90327\n",
      "question_indices: 90327\n",
      "yes_no_span: 90327\n",
      "answer_start_indices_offset: 90327\n",
      "answer_end_indices_offset: 90327\n"
     ]
    }
   ],
   "source": [
    "print(\"question_context_sequences: {}\".format(len(question_context_sequences)))\n",
    "print(\"question_ids: {}\".format(len(question_ids)))\n",
    "print(\"question_indices: {}\".format(len(question_indices)))\n",
    "print(\"yes_no_span: {}\".format(len(yes_no_span)))\n",
    "print(\"answer_start_indices_offset: {}\".format(len(answer_start_indices_offset)))\n",
    "print(\"answer_end_indices_offset: {}\".format(len(answer_end_indices_offset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {\n",
    "    \"question_context_sequences\": question_context_sequences,\n",
    "    \"question_ids\": question_ids,\n",
    "    \"question_indices\": question_indices,\n",
    "    \"yes_no_span\": yes_no_span,\n",
    "    \"answer_start_indices_offset\": answer_start_indices_offset,\n",
    "    \"answer_end_indices_offset\": answer_end_indices_offset,\n",
    "    \"segment_id\": segment_id,\n",
    "    \"max_question_len\":max_question_len,\n",
    "    \"max_seq_len\": max_seq_len,\n",
    "    \"ids_to_word_mappings\": ids_to_word_mappings\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_out_dict = {\n",
    "    \"question_context_sequences\": out_dict['question_context_sequences'][:small_dataset_size],\n",
    "    \"question_ids\": out_dict['question_ids'][:small_dataset_size],\n",
    "    \"question_indices\": out_dict['question_indices'][:small_dataset_size],\n",
    "    \"yes_no_span\": out_dict['yes_no_span'][:small_dataset_size],\n",
    "    \"answer_start_indices_offset\": out_dict['answer_start_indices_offset'][:small_dataset_size],\n",
    "    \"answer_end_indices_offset\": out_dict['answer_end_indices_offset'][:small_dataset_size],\n",
    "    \"segment_id\": out_dict['segment_id'],\n",
    "    \"max_question_len\": out_dict['max_question_len'],\n",
    "    \"max_seq_len\": out_dict['max_seq_len'],\n",
    "    \"ids_to_word_mappings\": out_dict[\"ids_to_word_mappings\"][:small_dataset_size]\n",
    "}    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict['answer_string'] = answers_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_out_dict['answer_string'] = answers_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(list(out_dict.keys()) == list(small_out_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(small_out_dict[\"question_context_sequences\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(small_out_dict[\"segment_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "pickler(out_pkl_path, small_out_pkl_name, small_out_dict)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "pickler(out_pkl_path, out_pkl_name, out_dict)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
