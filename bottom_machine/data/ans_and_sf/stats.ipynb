{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose: gather statistics on question lengths, passage lengths, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import pdb\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import (BasicTokenizer,\n",
    "                                                  BertTokenizer,\n",
    "                                                  whitespace_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:03<00:00, 67234.41B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickler(path,pkl_name,obj):\n",
    "    with open(os.path.join(path, pkl_name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def unpickler(path,pkl_name):\n",
    "    with open(os.path.join(path, pkl_name) ,'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "\n",
    "out_pkl_path = \"./\"\n",
    "\n",
    "if(TRAINING):\n",
    "    file_path = \"../../../hotpotqa/hotpot_train_v1.json\"\n",
    "#     out_pkl_name = \"preprocessed_train.pkl\"\n",
    "else:\n",
    "    file_path = \"../../../hotpotqa/hotpot_dev_distractor_v1.json\"\n",
    "#     out_pkl_name = \"preprocessed_dev.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, encoding='utf8') as file:\n",
    "    dataset = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7405/7405 [02:10<00:00, 56.70it/s]\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "paragraphs = [] \n",
    "supporting_facts = []\n",
    "\n",
    "\n",
    "for item_index, item in enumerate(tqdm(dataset)):\n",
    "#     if(item_index in problem_indices):\n",
    "#         continue\n",
    "    question = tokenize(item[\"question\"])\n",
    "    questions.append(question)\n",
    "    paragraph_names = []\n",
    "    paragraph_text = []\n",
    "    for i,para in enumerate(item[\"context\"]):\n",
    "        para_name = para[0]\n",
    "        para_sents = para[1]\n",
    "        paragraph_names.append(para_name)\n",
    "        paragraph_text.append([tokenize(s) for s in para_sents])\n",
    "    paragraphs.append(paragraph_text)\n",
    "    supp_fact_list = []\n",
    "    for sup_fact in item[\"supporting_facts\"]:\n",
    "        para_name = sup_fact[0]\n",
    "        supporting_fact_index = sup_fact[1] \n",
    "        para_index = paragraph_names.index(para_name)\n",
    "        supp_fact_list.append([para_index, supporting_fact_index])\n",
    "    supporting_facts.append(supp_fact_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg question len:19.590141796083728\n",
      "min question len:7\n",
      "max question len:65\n"
     ]
    }
   ],
   "source": [
    "question_lengths = np.array([len(q) for q in questions])\n",
    "\n",
    "print(\"Avg question len:{}\".format(question_lengths.mean()))\n",
    "print(\"min question len:{}\".format(question_lengths.min()))\n",
    "print(\"max question len:{}\".format(question_lengths.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06927751519243754"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_question_len = 30\n",
    "np.sum(np.greater(question_lengths,max_question_len))/question_lengths.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg document len:1194.0945307224847\n",
      "min document len:66\n",
      "max document len:3222\n"
     ]
    }
   ],
   "source": [
    "document_lengths = []\n",
    "\n",
    "for doc in paragraphs:\n",
    "    doc_len = 0\n",
    "    for para in doc:\n",
    "        for sent in para:\n",
    "            doc_len += len(sent)\n",
    "    document_lengths.append(doc_len)\n",
    "\n",
    "document_lengths = np.array(document_lengths)\n",
    "\n",
    "\n",
    "print(\"Avg document len:{}\".format(document_lengths.mean()))\n",
    "print(\"min document len:{}\".format(document_lengths.min()))\n",
    "print(\"max document len:{}\".format(document_lengths.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06144496961512492"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_document_len = 1700\n",
    "np.sum(np.greater(document_lengths,max_document_len))/document_lengths.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_num_chunks(max_sequence_length, max_question_length, max_document_length):\n",
    "    s1 = max_sequence_length - max_question_length\n",
    "    num_chunks = max_document_length / s1\n",
    "    return num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.549060542797495"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_num_chunks(max_sequence_length=509, max_question_length=30, max_document_length=1700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
