{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pdb\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import (BasicTokenizer,\n",
    "                                                  BertTokenizer,\n",
    "                                                  whitespace_tokenize)\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickler(path,pkl_name,obj):\n",
    "    with open(os.path.join(path, pkl_name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def unpickler(path,pkl_name):\n",
    "    with open(os.path.join(path, pkl_name) ,'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictedSpanFormatter:\n",
    "    \n",
    "    def __init__(self,max_answer_length):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.max_answer_length = max_answer_length #options.max_answer_length\n",
    "        \n",
    "    def un_tokenize(self,ids, tokens_to_text_mapping, bert_tokenizer):\n",
    "        out_list = []\n",
    "        start = 0\n",
    "        end = start\n",
    "        while (start < len(ids)) and (end < len(ids)):\n",
    "            i = len(ids)\n",
    "            decoded_anything = False\n",
    "            while (decoded_anything == False) and (i > start):\n",
    "                if(tuple(ids[start:i]) in tokens_to_text_mapping.keys()):\n",
    "                    out_list.append(tokens_to_text_mapping[tuple(ids[start:i])])\n",
    "                    decoded_anything = True\n",
    "                else:\n",
    "                    i -= 1\n",
    "            if(decoded_anything == False):\n",
    "                out_list.append(bert_tokenizer.convert_ids_to_tokens([ids[start]])[0])\n",
    "                start += 1\n",
    "                end = start\n",
    "            else:\n",
    "                start = i\n",
    "                end = i\n",
    "        return \" \".join(out_list)\n",
    "\n",
    "    def is_word_split(self,word):\n",
    "        if(len(word) < 2):\n",
    "            return False\n",
    "        else:\n",
    "            return (word[0] == '#' and word[1] == '#')\n",
    "\n",
    "    def combine_word_pieces(self, sentence):\n",
    "        # the first word cant start with '##'\n",
    "        out_tokens = []\n",
    "        for token in sentence:\n",
    "            if(not self.is_word_split(token)):\n",
    "                out_tokens.append((token))\n",
    "            else:\n",
    "                out_tokens[-1] += token[2:]\n",
    "        return out_tokens\n",
    "        \n",
    "    def convert_indices_to_text(self, sentence, start, end, tokens_to_text_mapping):\n",
    "        ''' (sentence, [10, 12]) --> ['runn', '##ing', 'race'] --> ['running', 'race']\n",
    "        --> \"running race\" '''\n",
    "        text = self.tokenizer.convert_ids_to_tokens(sentence)\n",
    "        true_start = start\n",
    "        if(self.is_word_split(text[start])):\n",
    "            for i in range(1,start):\n",
    "                if(not self.is_word_split(text[start-i])):\n",
    "                    true_start = start-i\n",
    "                    break\n",
    "        \n",
    "        true_end = end\n",
    "        for i in range(end+1, len(sentence)):\n",
    "            if(not self.is_word_split(text[i])):\n",
    "                true_end = i-1\n",
    "                break\n",
    "\n",
    "        proper_text = self.un_tokenize(sentence[true_start:true_end+1], tokens_to_text_mapping, self.tokenizer)\n",
    "#         proper_text = \" \".join(text[true_start:true_end+1]).replace('  ##','').replace(' ##','')\n",
    "        return proper_text\n",
    "        \n",
    "\n",
    "#     def find_most_confident_span(self, start_scores, end_scores):\n",
    "#         ''' \n",
    "#         Inputs: masked start_scores and end_scores of a single example\n",
    "#         Output: (i,j) pairs having highest Pr(i) + Pr(j)\n",
    "#         '''\n",
    "#         assert(len(start_scores) == len(end_scores))\n",
    "#         best_start = 0\n",
    "#         best_stop = 0\n",
    "#         best_confidence = 0\n",
    "#         for i in range(len(start_scores)):\n",
    "#             for j in range(i, min(len(end_scores), i + self.max_answer_length)):\n",
    "#                 if(start_scores[i] + end_scores[j] > best_confidence):\n",
    "#                     best_start = i\n",
    "#                     best_stop = j\n",
    "# #                     best_confidence = start_scores[i] + end_scores[j]\n",
    "#                     best_confidence = math.log(start_scores[i]) + math.log(end_scores[j])\n",
    "#         return best_start, best_stop\n",
    "    \n",
    "    def find_most_confident_span(self, start_scores, end_scores):\n",
    "        ''' \n",
    "        Inputs: masked start_scores and end_scores of a single example\n",
    "        Output: (i,j) pairs having highest Pr(i) + Pr(j)\n",
    "        '''\n",
    "        assert(len(start_scores) == len(end_scores))\n",
    "        best_start = 0\n",
    "        best_stop = 0\n",
    "        best_confidence = -1e100\n",
    "        for i in range(len(start_scores)):\n",
    "            for j in range(min(len(end_scores), i + self.max_answer_length)-1, i-1, -1):\n",
    "                if(math.log(start_scores[i]) + math.log(end_scores[j]) > best_confidence):\n",
    "                    best_start = i\n",
    "                    best_stop = j\n",
    "#                     best_confidence = start_scores[i] + end_scores[j]\n",
    "                    best_confidence = math.log(start_scores[i]) + math.log(end_scores[j])\n",
    "        return best_start, best_stop\n",
    "    \n",
    "    def find_top_n_confident_spans(self, start_scores, end_scores, n):\n",
    "        ''' \n",
    "        Inputs: masked start_scores and end_scores of a single example\n",
    "        Output: (i,j) n pairs having highest Pr(i) + Pr(j)\n",
    "        '''\n",
    "        assert(len(start_scores) == len(end_scores))\n",
    "        best_start = 0\n",
    "        best_stop = 0\n",
    "        best_confidence = -1e100\n",
    "        scores = []\n",
    "        for i in range(len(start_scores)):\n",
    "            for j in range(min(len(end_scores)-1, i + self.max_answer_length -1), i-1, -1):\n",
    "                s = math.log(start_scores[i]) + math.log(end_scores[j])\n",
    "                scores.append([s,i,j,start_scores[i],end_scores[j]])\n",
    "        scores.sort(key = lambda x:x[0], reverse = True)\n",
    "        return scores[:n]\n",
    "\n",
    "    def format_prediction(self, yes_no_span, start_scores, end_scores, \n",
    "                          sequences, tokens_to_text_mappings, \n",
    "                          question_ids, max_question_len,official_evalutation=True):\n",
    "        '''\n",
    "        input: all numpy arrays\n",
    "        output: {\"question_id\": answer_string}\n",
    "        '''\n",
    "        answers = {}\n",
    "        assert(len(yes_no_span) == len(start_scores) == len(end_scores) == len(sequences))\n",
    "        \n",
    "        #TODO use range instead of trange\n",
    "        for i in trange(len(yes_no_span)):\n",
    "            if(official_evalutation):\n",
    "                yns = yes_no_span[i].argmax(axis=-1)\n",
    "                if(yns == 0):\n",
    "                    answers[question_ids[i]] = \"yes\"\n",
    "                    continue\n",
    "                elif(yns == 1):\n",
    "                    answers[question_ids[i]] = \"no\"\n",
    "                    continue\n",
    "            \n",
    "            start, end = self.find_most_confident_span(start_scores[i], end_scores[i])\n",
    "            \n",
    "            sequence_chunks_concatenated = []\n",
    "            for seq in sequences[i]:\n",
    "                sequence_chunks_concatenated += seq[max_question_len + 2:]\n",
    "            \n",
    "            ans = self.convert_indices_to_text(sequence_chunks_concatenated, start, end, tokens_to_text_mappings[i])\n",
    "            answers[question_ids[i]] = ans\n",
    "        \n",
    "        assert(len(answers) == len(sequences))\n",
    "\n",
    "        return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportingFactFormatter:\n",
    "    '''\n",
    "    inputs: \n",
    "    - a binary array for each question. It will have 1 if the corresponding sentence is a supporting fact 0 otherwise.\n",
    "    - question id\n",
    "    - names of paragraphs in the context\n",
    "    - which paragraph is in which chunk\n",
    "    - number of sentences in each paragraph\n",
    "    \n",
    "    output:\n",
    "    A list like this\n",
    "    [['Bridgeport, Connecticut', 5], ['Brookhaven National Laboratory', 1]]\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, num_chunks, num_sentences_per_chunk):\n",
    "        self.num_chunks = num_chunks\n",
    "        self.num_sentences_per_chunk = num_sentences_per_chunk\n",
    "    \n",
    "    def find_all_indices(self, the_array, the_value):\n",
    "        assert(len(the_array.shape) == 1)\n",
    "        return list(np.where(the_array == the_value)[0])\n",
    "    \n",
    "    def find_paragraph_and_sentence_index(self, sent_index, paragraph_chunk_indices, num_sentences_in_paragraphs):\n",
    "        chunk_index = sent_index // self.num_sentences_per_chunk\n",
    "        assert(chunk_index < self.num_chunks)\n",
    "        sent_index = sent_index - (chunk_index * self.num_sentences_per_chunk)\n",
    "        num_sents_cum_sum = 0\n",
    "        para_index = -1\n",
    "        actual_sentence_index = -1\n",
    "        for p_index in paragraph_chunk_indices[chunk_index]:\n",
    "            if(num_sents_cum_sum <= sent_index < num_sents_cum_sum + num_sentences_in_paragraphs[p_index]):\n",
    "                para_index = p_index\n",
    "                actual_sentence_index = sent_index - num_sents_cum_sum\n",
    "                break\n",
    "            else:\n",
    "                num_sents_cum_sum += num_sentences_in_paragraphs[p_index]\n",
    "        return para_index, actual_sentence_index\n",
    "    \n",
    "    def find_paragraph_name(self, para_index, paragraph_names):\n",
    "        assert(0 <= para_index)\n",
    "        return paragraph_names[para_index]\n",
    "    \n",
    "    def format_supporting_facts(self, predictions, question_ids, \n",
    "                                paragraph_names, paragraph_chunk_indices, \n",
    "                                num_sentences_in_paragraphs):\n",
    "        assert( len(predictions) == len(question_ids) == len(paragraph_names) == len(paragraph_chunk_indices)\n",
    "               == len(num_sentences_in_paragraphs) )\n",
    "        \n",
    "        \n",
    "        out_records = {}\n",
    "        for i, pred_row in enumerate(predictions):\n",
    "            indices_of_sf = self.find_all_indices(the_array=pred_row, the_value=1)\n",
    "            formatted_sf_list = []\n",
    "            for sf_idx in indices_of_sf:\n",
    "                para_idx, sentence_idx = self.find_paragraph_and_sentence_index(sent_index = sf_idx, \n",
    "                                                     paragraph_chunk_indices = paragraph_chunk_indices[i], \n",
    "                                                     num_sentences_in_paragraphs=num_sentences_in_paragraphs[i])\n",
    "                if(para_idx < 0 or sentence_idx < 0):\n",
    "                    continue\n",
    "                para_name = self.find_paragraph_name(para_index=para_idx, paragraph_names=paragraph_names[i])\n",
    "                formatted_sf_list.append([para_name, sentence_idx])\n",
    "            out_records[question_ids[i]] = formatted_sf_list\n",
    "            \n",
    "        return out_records\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sff = SupportingFactFormatter(4, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sff.find_all_indices(the_array=np.array([1,2,1,2]), the_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sff.find_paragraph_and_sentence_index(sent_index=3, \n",
    "                                      paragraph_chunk_indices=[[0,1,2],[3,6],[4,5],[]], \n",
    "                                      num_sentences_in_paragraphs=[3,2,5,4,1,3,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1 2 3]\n",
      "1 [4 5 6]\n",
      "2 [7 8 9]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "\n",
    "for i,row in enumerate(a):\n",
    "    print(i , row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \n",
    "    '''Adapted from the official evaluation script'''\n",
    "    \n",
    "    def normalize_answer(self, s):\n",
    "\n",
    "        def remove_articles(text):\n",
    "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "        def white_space_fix(text):\n",
    "            return ' '.join(text.split())\n",
    "\n",
    "        def remove_punc(text):\n",
    "            exclude = set(string.punctuation)\n",
    "            return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "        def lower(text):\n",
    "            return text.lower()\n",
    "\n",
    "        return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "    def f1_score(self, prediction, ground_truth):\n",
    "        normalized_prediction = self.normalize_answer(prediction)\n",
    "        normalized_ground_truth = self.normalize_answer(ground_truth)\n",
    "\n",
    "        ZERO_METRIC = (0, 0, 0)\n",
    "\n",
    "        if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "            return ZERO_METRIC\n",
    "        if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "            return ZERO_METRIC\n",
    "\n",
    "        prediction_tokens = normalized_prediction.split()\n",
    "        ground_truth_tokens = normalized_ground_truth.split()\n",
    "        common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "        num_same = sum(common.values())\n",
    "        if num_same == 0:\n",
    "            return ZERO_METRIC\n",
    "        precision = 1.0 * num_same / len(prediction_tokens)\n",
    "        recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "        return f1, precision, recall\n",
    "\n",
    "\n",
    "    def exact_match_score(self, prediction, ground_truth):\n",
    "        return (self.normalize_answer(prediction) == self.normalize_answer(ground_truth))\n",
    "\n",
    "    def update_answer(self, metrics, prediction, gold):\n",
    "        em = self.exact_match_score(prediction, gold)\n",
    "        f1, prec, recall = self.f1_score(prediction, gold)\n",
    "        metrics['em'] += float(em)\n",
    "        metrics['f1'] += f1\n",
    "        metrics['prec'] += prec\n",
    "        metrics['recall'] += recall\n",
    "        return em, prec, recall\n",
    "\n",
    "    def update_sp(self, metrics, prediction, gold):\n",
    "        cur_sp_pred = set(map(tuple, prediction))\n",
    "        gold_sp_pred = set(map(tuple, gold))\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "        for e in cur_sp_pred:\n",
    "            if e in gold_sp_pred:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        for e in gold_sp_pred:\n",
    "            if e not in cur_sp_pred:\n",
    "                fn += 1\n",
    "        prec = 1.0 * tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "        recall = 1.0 * tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "        f1 = 2 * prec * recall / (prec + recall) if prec + recall > 0 else 0.0\n",
    "        em = 1.0 if fp + fn == 0 else 0.0\n",
    "        metrics['sp_em'] += em\n",
    "        metrics['sp_f1'] += f1\n",
    "        metrics['sp_prec'] += prec\n",
    "        metrics['sp_recall'] += recall\n",
    "        return em, prec, recall\n",
    "\n",
    "    def eval(self, prediction, gold):\n",
    "        metrics = {'em': 0, 'f1': 0, 'prec': 0, 'recall': 0,\n",
    "            'sp_em': 0, 'sp_f1': 0, 'sp_prec': 0, 'sp_recall': 0,\n",
    "            'joint_em': 0, 'joint_f1': 0, 'joint_prec': 0, 'joint_recall': 0}\n",
    "        for dp in gold:\n",
    "            cur_id = dp['_id']\n",
    "            can_eval_joint = True\n",
    "            if cur_id not in prediction['answer']:\n",
    "                print('missing answer {}'.format(cur_id))\n",
    "                can_eval_joint = False\n",
    "            else:\n",
    "                em, prec, recall = self.update_answer(\n",
    "                    metrics, prediction['answer'][cur_id], dp['answer'])\n",
    "            if cur_id not in prediction['sp']:\n",
    "                print('missing sp fact {}'.format(cur_id))\n",
    "                can_eval_joint = False\n",
    "            else:\n",
    "                sp_em, sp_prec, sp_recall = self.update_sp(\n",
    "                    metrics, prediction['sp'][cur_id], dp['supporting_facts'])            \n",
    "            \n",
    "            if can_eval_joint:\n",
    "                joint_prec = prec * sp_prec\n",
    "                joint_recall = recall * sp_recall\n",
    "                if joint_prec + joint_recall > 0:\n",
    "                    joint_f1 = 2 * joint_prec * joint_recall / (joint_prec + joint_recall)\n",
    "                else:\n",
    "                    joint_f1 = 0.\n",
    "                joint_em = em * sp_em\n",
    "\n",
    "                metrics['joint_em'] += joint_em\n",
    "                metrics['joint_f1'] += joint_f1\n",
    "                metrics['joint_prec'] += joint_prec\n",
    "                metrics['joint_recall'] += joint_recall\n",
    "\n",
    "        N = len(gold)\n",
    "        for k in metrics.keys():\n",
    "            metrics[k] /= N\n",
    "\n",
    "        print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerPredictionMaker:\n",
    "    \n",
    "    def prepare_span_predictions(self, data):\n",
    "        max_seq_len = (data['max_seq_len'] - data['max_question_len'] - 2) * data['num_chunks']\n",
    "        out_start_predictions = []\n",
    "        out_end_predictions = []\n",
    "        small_number = 1e-10\n",
    "        for i in range(len(data['answer_start_indices'])):\n",
    "\n",
    "            if(len(data['answer_start_indices'][i]) == 0):\n",
    "                start_indices = [small_number] * max_seq_len\n",
    "                end_indices = [small_number] * max_seq_len\n",
    "            else:\n",
    "                start_indices = [small_number] * max_seq_len\n",
    "                start_indices[data['answer_start_indices'][i][0]] = 1.0\n",
    "\n",
    "                end_indices = [small_number] * max_seq_len\n",
    "                end_indices[data['answer_end_indices'][i][0]] = 1.0\n",
    "\n",
    "            out_start_predictions.append(start_indices)\n",
    "            out_end_predictions.append(end_indices)\n",
    "\n",
    "        return np.array(out_start_predictions), np.array(out_end_predictions)\n",
    "    \n",
    "    def prepare_yes_no_span_pred(self, data):\n",
    "        out_yns = []\n",
    "        small_number = 1e-10\n",
    "        for i in range(len(data['yes_no_span'])):\n",
    "            yns = [small_number,small_number,small_number]\n",
    "            yns[data['yes_no_span'][i]] = 1\n",
    "            out_yns.append(yns)\n",
    "        return np.array(out_yns)\n",
    "    \n",
    "    \n",
    "    def preprare_predictions(self, raw_data):\n",
    "        start_predictions, end_predictions = self.prepare_span_predictions(raw_data)\n",
    "        yes_no_span_pred = self.prepare_yes_no_span_pred(raw_data)\n",
    "        \n",
    "        return yes_no_span_pred, start_predictions, end_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_gt_for_question_ids(data, question_ids):\n",
    "    out_records = []\n",
    "    for i, q_id in enumerate(question_ids):\n",
    "        record = {}\n",
    "        record['_id'] = q_id\n",
    "        question_index = data['question_ids'].index(q_id)\n",
    "        record['answer'] = data['answers_string'][question_index]\n",
    "        record['supporting_facts'] = data['supporting_facts_raw'][question_index]\n",
    "        out_records.append(record)\n",
    "    return out_records\n",
    "\n",
    "\n",
    "def prepare_gt_for_question_indices(data, question_indices=None):\n",
    "    if(question_indices == None):\n",
    "        question_indices = list(range(len(data['question_ids'])))\n",
    "    out_records = []\n",
    "    for i in question_indices:\n",
    "        record = {}\n",
    "        record['_id'] = data['question_ids'][i]\n",
    "        record['answer'] = data['answers_string'][i]\n",
    "        record['supporting_facts'] = data['supporting_facts_raw'][i]\n",
    "        out_records.append(record)\n",
    "    return out_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SFPredictionMaker:\n",
    "    \n",
    "#     def preprare_predictions(self, raw_data):\n",
    "#         out_list = []\n",
    "        \n",
    "#         for i in range(len(raw_data['question_indices'])):\n",
    "#             sf_merged = []\n",
    "#             for sf in raw_data['supporting_facts_expanded'][i]:\n",
    "#                 sf_merged += sf\n",
    "#             assert(len(sf_merged) == raw_data['num_chunks']*raw_data['max_num_sentences_per_chunk'])\n",
    "#             out_list.append(sf_merged)\n",
    "#         return np.array(out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = unpickler('./', 'preprocessed_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question_context_sequences', 'segment_id', 'sentence_start_indices', 'sentence_end_indices', 'answer_start_indices', 'answer_end_indices', 'supporting_facts_expanded', 'question_ids', 'question_indices', 'yes_no_span', 'ids_to_word_mappings', 'max_seq_len', 'max_question_len', 'max_num_sentences_per_chunk', 'num_chunks', 'paragraph_chunk_indices', 'num_sentences_in_paragraphs', 'paragraph_names', 'answers_string', 'supporting_facts_raw'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[89, 94]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['answer_start_indices'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['max_seq_len'] - raw_data['max_question_len'] - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_prediction_maker = AnswerPredictionMaker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_span_formatter = PredictedSpanFormatter(max_answer_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_formatter = SupportingFactFormatter(num_chunks=4, num_sentences_per_chunk=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_yns, pred_start_scores, pred_end_scores = answer_prediction_maker.preprare_predictions(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sf = np.array(raw_data['supporting_facts_expanded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90447, 72)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90447/90447 [27:11<00:00, 57.60it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_ans_str = predicted_span_formatter.format_prediction(pred_yns, pred_start_scores, pred_end_scores, \n",
    "                                                          sequences=raw_data['question_context_sequences'], \n",
    "                                                          tokens_to_text_mappings=raw_data['ids_to_word_mappings'], \n",
    "                                                          question_ids = raw_data['question_ids'],\n",
    "                                                          max_question_len = raw_data['max_question_len'],\n",
    "                                                          official_evalutation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "Original:  Arthur's Magazine\n",
      "Predicted:  Arthur's Magazine\n",
      "=================\n",
      "Original:  Delhi\n",
      "Predicted:  Delhi\n",
      "=================\n",
      "Original:  President Richard Nixon\n",
      "Predicted:  President Richard nixon\n",
      "=================\n",
      "Original:  American\n",
      "Predicted:  American\n",
      "=================\n",
      "Original:  alcohol\n",
      "Predicted:  alcohol\n",
      "=================\n",
      "Original:  Jonathan Stark\n",
      "Predicted:  Jonathan Stark\n",
      "=================\n",
      "Original:  Crambidae\n",
      "Predicted:  Crambidae\n",
      "=================\n",
      "Original:  Badr Hari\n",
      "Predicted:  Badr Hari\n",
      "=================\n",
      "Original:  2006\n",
      "Predicted:  2006\n",
      "=================\n",
      "Original:  6.213 km long\n",
      "Predicted:  6.213 km long\n"
     ]
    }
   ],
   "source": [
    "n=10\n",
    "for i in range(n):\n",
    "    q_id = raw_data['question_ids'][i]\n",
    "    print(\"=================\")\n",
    "    print(\"Original: \", raw_data['answers_string'][i])\n",
    "    print(\"Predicted: \", pred_ans_str[q_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sf_formatted = sf_formatter.format_supporting_facts(predictions = np.array(raw_data['supporting_facts_expanded']), \n",
    "                                                         question_ids = raw_data['question_ids'], \n",
    "                                                         paragraph_names = raw_data['paragraph_names'], \n",
    "                                                         paragraph_chunk_indices = raw_data['paragraph_chunk_indices'], \n",
    "                                                         num_sentences_in_paragraphs = raw_data['num_sentences_in_paragraphs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "Original:  [[\"Arthur's Magazine\", 0], ['First for Women', 0]]\n",
      "Predicted:  [['First for Women', 0], [\"Arthur's Magazine\", 0]]\n",
      "=================\n",
      "Original:  [['Oberoi family', 0], ['The Oberoi Group', 0]]\n",
      "Predicted:  [['Oberoi family', 0], ['The Oberoi Group', 0]]\n",
      "=================\n",
      "Original:  [['Allie Goertz', 0], ['Allie Goertz', 1], ['Allie Goertz', 2], ['Milhouse Van Houten', 0]]\n",
      "Predicted:  [['Milhouse Van Houten', 0], ['Allie Goertz', 0], ['Allie Goertz', 1], ['Allie Goertz', 2]]\n",
      "=================\n",
      "Original:  [['Peggy Seeger', 0], ['Peggy Seeger', 1], ['Ewan MacColl', 0]]\n",
      "Predicted:  [['Ewan MacColl', 0], ['Peggy Seeger', 0], ['Peggy Seeger', 1]]\n",
      "=================\n",
      "Original:  [['Cadmium chloride', 1], ['Ethanol', 0]]\n",
      "Predicted:  [['Cadmium chloride', 1], ['Ethanol', 0]]\n",
      "=================\n",
      "Original:  [['Jonathan Stark (tennis)', 0], ['Jonathan Stark (tennis)', 1], ['Henri Leconte', 1]]\n",
      "Predicted:  [['Jonathan Stark (tennis)', 0], ['Jonathan Stark (tennis)', 1], ['Henri Leconte', 1]]\n",
      "=================\n",
      "Original:  [['Indogrammodes', 0], ['Indogrammodes', 1], ['India', 0], ['India', 1]]\n",
      "Predicted:  [['Indogrammodes', 0], ['Indogrammodes', 1], ['India', 0], ['India', 1]]\n",
      "=================\n",
      "Original:  [['Global Fighting Championship', 1], ['Global Fighting Championship', 2], ['Badr Hari', 0], ['Badr Hari', 2]]\n",
      "Predicted:  [['Global Fighting Championship', 1], ['Global Fighting Championship', 2], ['Badr Hari', 0], ['Badr Hari', 2]]\n",
      "=================\n",
      "Original:  [['House of Anubis', 0], ['Het Huis Anubis', 1]]\n",
      "Predicted:  [['Het Huis Anubis', 1], ['House of Anubis', 0]]\n",
      "=================\n",
      "Original:  [['2013 Liqui Moly Bathurst 12 Hour', 0], ['2013 Liqui Moly Bathurst 12 Hour', 1], ['Mount Panorama Circuit', 0], ['Mount Panorama Circuit', 2]]\n",
      "Predicted:  [['Mount Panorama Circuit', 0], ['Mount Panorama Circuit', 2], ['2013 Liqui Moly Bathurst 12 Hour', 0], ['2013 Liqui Moly Bathurst 12 Hour', 1]]\n"
     ]
    }
   ],
   "source": [
    "n=10\n",
    "for i in range(n):\n",
    "    q_id = raw_data['question_ids'][i]\n",
    "    print(\"=================\")\n",
    "    print(\"Original: \", raw_data['supporting_facts_raw'][i])\n",
    "    print(\"Predicted: \", pred_sf_formatted[q_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_predictions = {'answer':pred_ans_str, 'sp':pred_sf_formatted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_sp = {}\n",
    "\n",
    "# for i in range(len(raw_data['question_ids'])):\n",
    "#     gt_sp[raw_data['question_ids'][i]] = raw_data['supporting_facts_raw'][i]\n",
    "\n",
    "# formatted_predictions = {'answer':pred_ans_str, 'sp':gt_sp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# massaged_gt = prepare_gt_for_question_ids(data=raw_data, question_ids = raw_data['question_ids'])\n",
    "massaged_gt = prepare_gt_for_question_indices(data=raw_data, question_indices=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'em': 0.939677380123166, 'f1': 0.9573750549094912, 'prec': 0.956260543619155, 'recall': 0.9606650393543146, 'sp_em': 0.9935874047784891, 'sp_f1': 0.9981272026713048, 'sp_prec': 0.9999391909073823, 'sp_recall': 0.997172456166122, 'joint_em': 0.934912158501664, 'joint_f1': 0.9559789282733778, 'joint_prec': 0.9562135547748595, 'joint_recall': 0.9585402653848422}\n"
     ]
    }
   ],
   "source": [
    "evaluator.eval(prediction=formatted_predictions, gold=massaged_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
