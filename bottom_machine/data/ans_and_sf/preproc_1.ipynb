{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purporse: Read the data, tokenize it and store it in a format which can be massaged as needed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import pdb\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import (BasicTokenizer,\n",
    "                                                  BertTokenizer,\n",
    "                                                  whitespace_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickler(path,pkl_name,obj):\n",
    "    with open(os.path.join(path, pkl_name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def unpickler(path,pkl_name):\n",
    "    with open(os.path.join(path, pkl_name) ,'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = True\n",
    "\n",
    "out_pkl_path = \"./\"\n",
    "\n",
    "if(TRAINING):\n",
    "    file_path = \"../../../hotpotqa/hotpot_train_v1.1.json\"\n",
    "    out_pkl_name = \"preproc_train_1.pkl\"\n",
    "else:\n",
    "    file_path = \"../../../hotpotqa/hotpot_dev_distractor_v1.json\"\n",
    "    out_pkl_name = \"preproc_dev_1.pkl\"\n",
    "\n",
    "# max_seq_len = 510\n",
    "# max_num_paragraphs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, encoding='utf8') as file:\n",
    "    dataset = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, tokens_to_text_mapping, bert_tokenizer):\n",
    "    out_list = []\n",
    "    tokens = whitespace_tokenize(text)\n",
    "    for tok in tokens:\n",
    "        ids = bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize(tok))\n",
    "        tokens_to_text_mapping[tuple(ids)] = tok\n",
    "        out_list += ids\n",
    "    return out_list\n",
    "\n",
    "def un_tokenize(ids, tokens_to_text_mapping, bert_tokenizer):\n",
    "    out_list = []\n",
    "    start = 0\n",
    "    end = start\n",
    "    while (start < len(ids)) and (end < len(ids)):\n",
    "        i = len(ids)\n",
    "        decoded_anything = False\n",
    "        while (decoded_anything == False) and (i > start):\n",
    "            if(tuple(ids[start:i]) in tokens_to_text_mapping.keys()):\n",
    "                out_list.append(tokens_to_text_mapping[tuple(ids[start:i])])\n",
    "                decoded_anything = True\n",
    "            else:\n",
    "                i -= 1\n",
    "        if(decoded_anything == False):\n",
    "            start += 1\n",
    "            end = start\n",
    "        else:\n",
    "            start = i\n",
    "            end = i\n",
    "    return \" \".join(out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90447/90447 [33:03<00:00, 45.59it/s]\n"
     ]
    }
   ],
   "source": [
    "question_ids = []\n",
    "questions = []\n",
    "paragraphs = []\n",
    "paragraph_names = []\n",
    "answers = []\n",
    "answers_string = []\n",
    "question_indices = []\n",
    "yes_no_span = []\n",
    "supporting_facts = []\n",
    "ids_to_word_mappings = []\n",
    "supporting_facts_raw = []\n",
    "skipped = []\n",
    "\n",
    "for item_index, item in enumerate(tqdm(dataset)):\n",
    "    answers_string.append(item[\"answer\"])\n",
    "    id_to_word = {}\n",
    "    para_names = []\n",
    "    para_text = []\n",
    "    for i,para in enumerate(item[\"context\"]):\n",
    "        p_name = para[0]\n",
    "        p_sents = para[1]\n",
    "        p_sents[0] = p_name + \". \" +p_sents[0]\n",
    "        para_names.append(p_name)\n",
    "        para_text.append([tokenize(s, id_to_word, tokenizer) for s in p_sents])\n",
    "    paragraphs.append(para_text)\n",
    "    paragraph_names.append(para_names)\n",
    "    supp_fact_list = []\n",
    "    supporting_facts_raw.append(item[\"supporting_facts\"])\n",
    "    for sup_fact in item[\"supporting_facts\"]:\n",
    "        p_name = sup_fact[0]\n",
    "        supporting_fact_index = sup_fact[1] \n",
    "        para_index = para_names.index(p_name)\n",
    "        supp_fact_list.append([para_index, supporting_fact_index])\n",
    "    \n",
    "    supporting_facts.append(supp_fact_list)\n",
    "    question_indices.append(item_index)\n",
    "    question_ids.append(item[\"_id\"])\n",
    "    question = tokenize(item[\"question\"], id_to_word, tokenizer)\n",
    "    questions.append(question)\n",
    "    answer_str = item[\"answer\"]\n",
    "    if(answer_str == \"yes\"):\n",
    "        yes_no_span.append(0)\n",
    "    elif(answer_str == \"no\"):\n",
    "        yes_no_span.append(1)\n",
    "    else:\n",
    "        yes_no_span.append(2)\n",
    "    answer_tokenized = tokenize(answer_str, {}, tokenizer)\n",
    "    answers.append(answer_tokenized)\n",
    "    ids_to_word_mappings.append(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(question_ids) ==\n",
    "len(questions) ==\n",
    "len(paragraphs) == \n",
    "len(paragraph_names) == \n",
    "len(answers) == \n",
    "len(question_indices) == \n",
    "len(yes_no_span) == \n",
    "len(supporting_facts) == \n",
    "len(ids_to_word_mappings) ==\n",
    "len(supporting_facts_raw) == \n",
    "len(answers_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90447"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {\n",
    "    \"question_ids\" : question_ids,\n",
    "    \"questions\" : questions,\n",
    "    \"paragraphs\" : paragraphs,\n",
    "    \"paragraph_names\" : paragraph_names,\n",
    "    \"answers\" : answers,\n",
    "    \"question_indices\" : question_indices,\n",
    "    \"yes_no_span\" : yes_no_span,\n",
    "    \"supporting_facts\" : supporting_facts,\n",
    "    \"ids_to_word_mappings\" : ids_to_word_mappings,\n",
    "    \"answers_string\" : answers_string,\n",
    "    \"supporting_facts_raw\": supporting_facts_raw\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_paras = Counter([len(p) for p in paragraphs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({10: 89609, 8: 60, 2: 262, 6: 53, 3: 156, 5: 88, 4: 94, 7: 77, 9: 48})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"Arthur's Magazine\", 0], ['First for Women', 0]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supporting_facts_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "pickler(out_pkl_path, out_pkl_name, out_dict)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
